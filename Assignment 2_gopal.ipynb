{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92778525",
   "metadata": {},
   "source": [
    "# Assignment 2: Linear Models and Validation Metrics (30 marks total)\n",
    "### Due: October 10 at 11:59pm\n",
    "\n",
    "### Name: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce31b39a",
   "metadata": {},
   "source": [
    "### In this assignment, you will need to write code that uses linear models to perform classification and regression tasks. You will also be asked to describe the process by which you came up with the code. More details can be found below. Please cite any websites or AI tools that you used to help you with this assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c6de86",
   "metadata": {},
   "source": [
    "## Part 1: Classification (14.5 marks total)\n",
    "\n",
    "You have been asked to develop code that can help the user determine if the email they have received is spam or not. Following the machine learning workflow described in class, write the relevant code in each of the steps below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3c6fc8",
   "metadata": {},
   "source": [
    "### Step 0: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "33f86925",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9d33a8",
   "metadata": {},
   "source": [
    "### Step 1: Data Input (1 mark)\n",
    "\n",
    "The data used for this task can be downloaded using the yellowbrick library: \n",
    "https://www.scikit-yb.org/en/latest/api/datasets/spam.html\n",
    "\n",
    "Use the yellowbrick function `load_spam()` to load the spam dataset into the feature matrix `X` and target vector `y`.\n",
    "\n",
    "Print the size and type of `X` and `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "33583c67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of X:  (4600, 57)\n",
      "Type of X:  <class 'pandas.core.frame.DataFrame'>\n",
      "Size of y:  (4600,)\n",
      "Type of y:  <class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "# TO DO: Import spam dataset from yellowbrick library\n",
    "\n",
    "from yellowbrick.datasets import load_spam\n",
    "X, y = load_spam()\n",
    "# TO DO: Print size and type of X and y\n",
    "print(\"Size of X: \" , X.shape)\n",
    "print(\"Type of X: \", type(X))\n",
    "print(\"Size of y: \" , y.shape)\n",
    "print(\"Type of y: \", type(y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156db208",
   "metadata": {},
   "source": [
    "### Step 2: Data Processing (1.5 marks)\n",
    "\n",
    "Check to see if there are any missing values in the dataset. If necessary, select an appropriate method to fill-in the missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4e7204f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_freq_make                0\n",
      "word_freq_address             0\n",
      "word_freq_all                 0\n",
      "word_freq_3d                  0\n",
      "word_freq_our                 0\n",
      "word_freq_over                0\n",
      "word_freq_remove              0\n",
      "word_freq_internet            0\n",
      "word_freq_order               0\n",
      "word_freq_mail                0\n",
      "word_freq_receive             0\n",
      "word_freq_will                0\n",
      "word_freq_people              0\n",
      "word_freq_report              0\n",
      "word_freq_addresses           0\n",
      "word_freq_free                0\n",
      "word_freq_business            0\n",
      "word_freq_email               0\n",
      "word_freq_you                 0\n",
      "word_freq_credit              0\n",
      "word_freq_your                0\n",
      "word_freq_font                0\n",
      "word_freq_000                 0\n",
      "word_freq_money               0\n",
      "word_freq_hp                  0\n",
      "word_freq_hpl                 0\n",
      "word_freq_george              0\n",
      "word_freq_650                 0\n",
      "word_freq_lab                 0\n",
      "word_freq_labs                0\n",
      "word_freq_telnet              0\n",
      "word_freq_857                 0\n",
      "word_freq_data                0\n",
      "word_freq_415                 0\n",
      "word_freq_85                  0\n",
      "word_freq_technology          0\n",
      "word_freq_1999                0\n",
      "word_freq_parts               0\n",
      "word_freq_pm                  0\n",
      "word_freq_direct              0\n",
      "word_freq_cs                  0\n",
      "word_freq_meeting             0\n",
      "word_freq_original            0\n",
      "word_freq_project             0\n",
      "word_freq_re                  0\n",
      "word_freq_edu                 0\n",
      "word_freq_table               0\n",
      "word_freq_conference          0\n",
      "char_freq_;                   0\n",
      "char_freq_(                   0\n",
      "char_freq_[                   0\n",
      "char_freq_!                   0\n",
      "char_freq_$                   0\n",
      "char_freq_#                   0\n",
      "capital_run_length_average    0\n",
      "capital_run_length_longest    0\n",
      "capital_run_length_total      0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# TO DO: Check if there are any missing values and fill them in if necessary\n",
    "anyMissingValue = X.isnull().sum()\n",
    "print(anyMissingValue)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a489285a",
   "metadata": {},
   "source": [
    "For this task, we want to test if the linear model would still work if we used less data. Use the `train_test_split` function from sklearn to create a new feature matrix named `X_small` and a new target vector named `y_small` that contain **5%** of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f9bc4a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Create X_small and y_small \n",
    "from sklearn.model_selection import train_test_split\n",
    "X_small_train, X_small_test, y_small_train, y_small_test  = train_test_split(X, y, random_state = 0, train_size = 0.05) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e6c46f",
   "metadata": {},
   "source": [
    "### Step 3: Implement Machine Learning Model\n",
    "\n",
    "1. Import `LogisticRegression` from sklearn\n",
    "2. Instantiate model `LogisticRegression(max_iter=2000)`.\n",
    "3. Implement the machine learning model with three different datasets: \n",
    "    - `X` and `y`\n",
    "    - Only first two columns of `X` and `y`\n",
    "    - `X_small` and `y_small`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "85ba6794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1 Importing LogisticRegression from sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 3.2 Instantiate model LogisticRegression(max_iter = 2000)\n",
    "logreg = LogisticRegression(max_iter=2000)\n",
    "\n",
    "# 3.3 Implement the machine learning model with three different datasets\n",
    "# a) set for  X and y\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0)\n",
    "\n",
    "# b) For only two columns of X and y\n",
    "X_subset = X.iloc[:, :2]\n",
    "X_train_selective, X_test_selective, y_train_selective, y_test_selective = train_test_split(X_subset, y, random_state = 0)\n",
    "\n",
    "# c) For  X_small and y_small\n",
    "\n",
    "X_small_train, X_small_test, y_small_train, y_small_test  = train_test_split(X, y, random_state = 0, train_size = 0.05) \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89f3d84",
   "metadata": {},
   "source": [
    "### Step 4: Validate Model\n",
    "\n",
    "Calculate the training and validation accuracy for the three different tests implemented in Step 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5dd185b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score for complete set: 0.929\n",
      "Validation set score for complete set: 0.938\n",
      "\n",
      "Training set score for set(contains first two columns): 0.608\n",
      "Validation set score for set(contains first two columns): 0.613\n",
      "\n",
      "Training set score for set(that contains 5 percentage of the data): 0.948\n",
      "Validation set score for set(that contains 5 percentage of the data): 0.907\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Validation for the first set(compkete Data Set)\n",
    "logreg1 = LogisticRegression(max_iter=2000).fit(X_train, y_train)\n",
    "print(\"Training set score for complete set: {:.3f}\".format(logreg1.score(X_train, y_train)))\n",
    "print(\"Validation set score for complete set: {:.3f}\".format(logreg1.score(X_test, y_test)))\n",
    "print('')\n",
    "\n",
    "\n",
    "# Validation for the second set(Data set that contains First two columns)\n",
    "logreg2 = LogisticRegression(max_iter=2000).fit(X_train_selective, y_train_selective)\n",
    "print(\"Training set score for set(contains first two columns): {:.3f}\".format(logreg2.score(X_train_selective, y_train_selective)))\n",
    "print(\"Validation set score for set(contains first two columns): {:.3f}\".format(logreg2.score(X_test_selective, y_test_selective)))\n",
    "print('')\n",
    "\n",
    "\n",
    "# Validation for the first set()(Data set that contains 5% of the data)\n",
    "logreg3 = LogisticRegression(max_iter=2000).fit(X_small_train, y_small_train)\n",
    "print(\"Training set score for set(that contains 5 percentage of the data): {:.3f}\".format(logreg3.score(X_small_train, y_small_train)))\n",
    "print(\"Validation set score for set(that contains 5 percentage of the data): {:.3f}\".format(logreg3.score(X_small_test, y_small_test)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352106a3",
   "metadata": {},
   "source": [
    "### Step 5: Visualize Results (4 marks)\n",
    "\n",
    "1. Create a pandas DataFrame `results` with columns: Data size, training accuracy, validation accuracy\n",
    "2. Add the data size, training and validation accuracy for each dataset to the `results` DataFrame\n",
    "3. Print `results`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "be4b5c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Data size  training accuracy  validation accuracy\n",
      "0       3450           0.928696             0.938261\n",
      "1       3450           0.608406             0.613043\n",
      "2        230           0.947826             0.906636\n"
     ]
    }
   ],
   "source": [
    "# TO DO: ADD YOUR CODE HERE FOR STEPS 3-5\n",
    "# Note: for any random state parameters, you can use random_state = 0\n",
    "# HINT: USING A LOOP TO STORE THE DATA IN YOUR RESULTS DATAFRAME WILL BE MORE EFFICIENT\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "data = {\n",
    "  'Data size' : [len(X_train),len(X_train_selective),len(X_small_train)],\n",
    "  'training accuracy' : [logreg1.score(X_train, y_train),logreg2.score(X_train_selective, y_train_selective), logreg3.score(X_small_train, y_small_train)],\n",
    "  'validation accuracy' : [logreg1.score(X_test, y_test), logreg2.score(X_test_selective, y_test_selective), logreg3.score(X_small_test, y_small_test)  ]\n",
    "}\n",
    "results = pd.DataFrame(data)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4427d4f",
   "metadata": {},
   "source": [
    "### Questions (4 marks)\n",
    "1. How do the training and validation accuracy change depending on the amount of data used? Explain with values.\n",
    "2. In this case, what do a false positive and a false negative represent? Which one is worse?\n",
    "\n",
    "*YOUR ANSWERS HERE*\n",
    "Answer 1- Full Data Set (3450 samples):\n",
    "Training Accuracy: 92.87%\n",
    "Validation Accuracy: 93.83%\n",
    "\n",
    "First Two Columns of Data (3450 samples):\n",
    "Training Accuracy: 60.84%\n",
    "Validation Accuracy: 61.30%\n",
    "\n",
    "Only 5% of the Data (230 samples):\n",
    "Training Accuracy: 94.78%\n",
    "Validation Accuracy: 90.66%\n",
    "\n",
    "I observed that , Training accuracy tends to remain high even with a smaller data size(comparing 92.87% with full data vs 94.78% with 5% of data), indicating that the model can fit the available data well. However, Validation accuracy may slightly decrease (from 93.83% to 90.66%) with smaller data sizes but can still be relatively high if the subset used for validation is representative.\n",
    "On the other hand, using a limited subset of features (first two columns) can lead to a significant drop in accuracy(Training Accuracy: 60.84% and Validation Accuracy: 61.30% ), suggesting the importance of feature selection.\n",
    "\n",
    "\n",
    "Answer 2- False Positive : A false positive occurs when the spam filter incorrectly classifies a legitimate email as spam. \n",
    "False Negative : A false negative occurs when the spam filter incorrectly classifies a spam email as legitimate\n",
    "In my opinion, the consequence of a false negative is potentially worse. It means that unwanted spam emails are not filtered out and may clutter the user's inbox. Some of these spam emails could be phishing attempts or contain malware, posing security risks to the user.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7559517a",
   "metadata": {},
   "source": [
    "### Process Description (4 marks)\n",
    "Please describe the process you used to create your code. Cite any websites or generative AI tools used. You can use the following questions as guidance:\n",
    "1. Where did you source your code?\n",
    "1. In what order did you complete the steps?\n",
    "1. If you used generative AI, what prompts did you use? Did you need to modify the code at all? Why or why not?\n",
    "1. Did you have any challenges? If yes, what were they? If not, what helped you to be successful?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59fe687f",
   "metadata": {},
   "source": [
    "*DESCRIBE YOUR PROCESS HERE*\n",
    "Q1 Where did you source your code?\n",
    "Answer:- The main source of the code was the practice example given by the instructor in D2L. In addition, I used ChatGPT to learn some of the instructions.\n",
    "\n",
    "Q2 In what order did you complete the steps?\n",
    "Answer :- 1. Loading and arrange data into feature matrix and target vector\n",
    "2. Checking any missing values\n",
    "3. Choose model class\n",
    "4. Instantiating model and selecting hyperparameters\n",
    "5.Fitting model to data\n",
    "6.Predict values for new data\n",
    "7. Comparing accuracy.\n",
    "\n",
    "Q3 If you used generative AI, what prompts did you use? Did you need to modify the code at all? Why or why not?\n",
    "Answer: - I used some of the prompts like:-\n",
    "a. \tWhat are touple object and how to check it, can you give me an example of touple in dataset?\n",
    "b. \tHow to have subset of dataset?  And I modified the code accordingly.\n",
    "c. \tHow to make a loop to insert the data values in the frame\n",
    "I had to make some changes to my code as I was not able to take subset of data.\n",
    "\n",
    "Q4 Did you have any challenges? If yes, what were they? If not, what helped you to be successful?\n",
    "Answer- Yes, I faced some difficulties like I was not able to split the data or how the regression models works. Then, I took the help of lecture notes and examples on D2L.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4c78a8",
   "metadata": {},
   "source": [
    "## Part 2: Regression (10.5 marks total)\n",
    "\n",
    "For this section, we will be evaluating concrete compressive strength of different concrete samples, based on age and ingredients. You will need to repeat the steps 1-4 from Part 1 for this analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ba83c5",
   "metadata": {},
   "source": [
    "### Step 1: Data Input (1 mark)\n",
    "\n",
    "The data used for this task can be downloaded using the yellowbrick library: \n",
    "https://www.scikit-yb.org/en/latest/api/datasets/concrete.html\n",
    "\n",
    "Use the yellowbrick function `load_concrete()` to load the spam dataset into the feature matrix `X` and target vector `y`.\n",
    "\n",
    "Print the size and type of `X` and `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6ff2e34f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of X:  (1030, 8)\n",
      "Type of X:  <class 'pandas.core.frame.DataFrame'>\n",
      "Size of y:  (1030,)\n",
      "Type of y:  <class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "# TO DO: Import spam dataset from yellowbrick library\n",
    "from yellowbrick.datasets import load_concrete\n",
    "X,y = load_concrete()\n",
    "# TO DO: Print size and type of X and y\n",
    "print(\"Size of X: \" , X.shape)\n",
    "print(\"Type of X: \", type(X))\n",
    "print(\"Size of y: \" , y.shape)\n",
    "print(\"Type of y: \", type(y))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5294cfa",
   "metadata": {},
   "source": [
    "### Step 2: Data Processing (0.5 marks)\n",
    "\n",
    "Check to see if there are any missing values in the dataset. If necessary, select an appropriate method to fill-in the missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "693c5fa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cement    0\n",
      "slag      0\n",
      "ash       0\n",
      "water     0\n",
      "splast    0\n",
      "coarse    0\n",
      "fine      0\n",
      "age       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# TO DO: Check if there are any missing values and fill them in if necessary\n",
    "anyMissingValue = X.isnull().sum()\n",
    "print(anyMissingValue)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc60489",
   "metadata": {},
   "source": [
    "### Step 3: Implement Machine Learning Model (1 mark)\n",
    "\n",
    "1. Import `LinearRegression` from sklearn\n",
    "2. Instantiate model `LinearRegression()`.\n",
    "3. Implement the machine learning model with `X` and `y`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b5041945",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TO DO: ADD YOUR CODE HERE\n",
    "# Note: for any random state parameters, you can use random_state = 0\n",
    "\n",
    "# 1. Importing LinearRegression from sklearn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# 2.  Instantiate model LogisticRegression(max_iter = 2000)\n",
    "lr = LinearRegression()\n",
    "\n",
    "# 3. Implement the machine learning model with X and y\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0)\n",
    "\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de28482",
   "metadata": {},
   "source": [
    "### Step 4: Validate Model (1 mark)\n",
    "\n",
    "Calculate the training and validation accuracy using mean squared error and R2 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "970c038b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Training score: 111.36\n",
      "Mean Squared Validation score: 95.90\n",
      "\n",
      "R2 Training score: 0.61\n",
      "R2 Valdiation score: 0.62\n"
     ]
    }
   ],
   "source": [
    "# TO DO: ADD YOUR CODE HERE\n",
    "print(\"Mean Squared Training score: {:.2f}\".format(mean_squared_error(y_train, lr.predict(X_train))))\n",
    "print(\"Mean Squared Validation score: {:.2f}\".format(mean_squared_error(y_test, lr.predict(X_test))))\n",
    "print('')\n",
    "print(\"R2 Training score: {:.2f}\".format(r2_score(y_train, lr.predict(X_train))))\n",
    "print(\"R2 Valdiation score: {:.2f}\".format(r2_score(y_test, lr.predict(X_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54aa7795",
   "metadata": {},
   "source": [
    "### Step 5: Visualize Results (1 mark)\n",
    "1. Create a pandas DataFrame `results` with columns: Training accuracy and Validation accuracy, and index: MSE and R2 score\n",
    "2. Add the accuracy results to the `results` DataFrame\n",
    "3. Print `results`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "88d223f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Training Accuracy  Validation Accuracy\n",
      "MSE                  111.36                95.90\n",
      "R2 score               0.61                 0.62\n"
     ]
    }
   ],
   "source": [
    "# TO DO: ADD YOUR CODE HERE\n",
    "data = {\n",
    "  'Training Accuracy' : [round(mean_squared_error(y_train, lr.predict(X_train)), 2), round(r2_score(y_train, lr.predict(X_train)), 2)],\n",
    "  'Validation Accuracy' : [round(mean_squared_error(y_test, lr.predict(X_test)), 2), round(r2_score(y_test, lr.predict(X_test)), 2) ]\n",
    "}\n",
    "results = pd.DataFrame(data, index = ['MSE', 'R2 score'])\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a42bda",
   "metadata": {},
   "source": [
    "### Questions (2 marks)\n",
    "1. Did using a linear model produce good results for this dataset? Why or why not?\n",
    "Answer:- In this case, the linear model didn't perform too well. The Mean Squared Error (MSE) for training was about 111.36, and for validation, it was around 95.90 . The R2 scores, which show how well the model explains the data, were only about 0.61 for training and 0.62 for validation.\n",
    "These numbers suggest that the linear model struggled to make accurate predictions. The high MSE means the model's predictions were quite off, and the R2 scores indicate that it could only explain around 60% of the data's variance.\n",
    "So, using a linear model didn't produce good results for this dataset. It might be better to try other models or approaches to improve accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca0ff2f",
   "metadata": {},
   "source": [
    "### Process Description (4 marks)\n",
    "Please describe the process you used to create your code. Cite any websites or generative AI tools used. You can use the following questions as guidance:\n",
    "1. Where did you source your code?\n",
    "1. In what order did you complete the steps?\n",
    "1. If you used generative AI, what prompts did you use? Did you need to modify the code at all? Why or why not?\n",
    "1. Did you have any challenges? If yes, what were they? If not, what helped you to be successful?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfdb0880",
   "metadata": {},
   "source": [
    "*DESCRIBE YOUR PROCESS HERE*\n",
    "Q1.  Where did you source your code?\n",
    "Answer:- The main source of the code was the practice example given by the instructor in D2L. In addition, I used ChatGPT to learn some of the instructions.\n",
    "\n",
    "Q2.  In what order did you complete the steps?\n",
    "Answer :- 1. Loading and arrange data into feature matrix and target vector\n",
    "2. Checking any missing values\n",
    "3. Choose model class\n",
    "4. Instantiating model and selecting hyperparameters\n",
    "5.Fitting model to data\n",
    "6.Predict values for new data\n",
    "7. Comparing accuracy.\n",
    "\n",
    "Q3.  If you used generative AI, what prompts did you use? Did you need to modify the code at all? Why or why not?\n",
    "Answer: - I used some of the prompts like:-What are mean square error and R2 Score and how with these parameters define the model\n",
    "Q4.  Did you have any challenges? If yes, what were they? If not, what helped you to be successful?\n",
    "Answer- Yes, I faced some difficulties like I was not able to understand R2 score then I took help from google as well..\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72ac3eb",
   "metadata": {},
   "source": [
    "## Part 3: Observations/Interpretation (3 marks)\n",
    "\n",
    "Describe any pattern you see in the results. Relate your findings to what we discussed during lectures. Include data to justify your findings.\n",
    "\n",
    "\n",
    "*ADD YOUR FINDINGS HERE*\n",
    "Part 1- Full Data Set vs. First Two Columns:\n",
    "Using the full data set yields high training and validation accuracy, indicating good performance.Using only the first two columns significantly drops accuracy, suggesting the need for more data or informative features.\n",
    "Full Data Set vs. 5% of the Data:\n",
    "Even with only 5% of the data, training accuracy remains high.Validation accuracy, though slightly lower, is still good, implying that this small subset may contain critical patterns.\n",
    "Part 2- \"When analyzing the results, a noticeable pattern emerges. The Mean Squared Error (MSE) values for both training and validation data are relatively high, with training at around 111.36 and validation around 95.90. Additionally, the R2 scores, which indicate how well the model fits the data, are approximately 0.61 for training and 0.62 for validation.These findings are consistent with what we discussed during lectures. A high MSE suggests that the linear model struggled to make accurate predictions, and the R2 scores, though not extremely low, indicate that the model could only explain around 60% of the data's variance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b84eed",
   "metadata": {},
   "source": [
    "## Part 4: Reflection (2 marks)\n",
    "Include a sentence or two about:\n",
    "- what you liked or disliked,\n",
    "- found interesting, confusing, challangeing, motivating\n",
    "while working on this assignment.\n",
    "\n",
    "\n",
    "*ADD YOUR THOUGHTS HERE*\n",
    "I liked the way this assignment is designed. While making this assignment , I revised all the components that we have learnt. The challanging thing was to cover the MSE and R2 score part"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db951b3a",
   "metadata": {},
   "source": [
    "## Part 5: Bonus Question (4 marks)\n",
    "\n",
    "Repeat Part 2 with Ridge and Lasso regression to see if you can improve the accuracy results. Which method and what value of alpha gave you the best R^2 score? Is this score \"good enough\"? Explain why or why not.\n",
    "\n",
    "**Remember**: Only test values of alpha from 0.001 to 100 along the logorithmic scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "47623d44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 Training score for Ridge model: 0.56\n",
      "R2 Valdiation score for Ridge model: 0.54\n",
      "R2 Training score for Lasso model: 0.55\n",
      "R2 Valdiation score for Lasso model: 0.54\n"
     ]
    }
   ],
   "source": [
    "# TO DO: ADD YOUR CODE HERE\n",
    "# 1 ) With Ridge regression \n",
    "# 1.a. Importing RidgeRegression from sklearn\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# 1.b.  Instantiate model Ridge model with an appropriate alpha value\n",
    "ridge = Ridge(alpha=1)   # alpha value from 0.001 to 37 gives the same value after that it start decreasing \n",
    "\n",
    "# 1.c. Implement the machine learning model with X and y\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0)\n",
    "\n",
    "ridge.fit(X_train, y_train)\n",
    "\n",
    "print(\"R2 Training score for Ridge model: {:.2f}\".format(r2_score(y_train, ridge.predict(X_train))))\n",
    "print(\"R2 Valdiation score for Ridge model: {:.2f}\".format(r2_score(y_test, ridge.predict(X_test))))\n",
    "\n",
    "# 2 ) With Lasso regression \n",
    "# 2.a. Importing LassoRegression from sklearn\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# 2.b.  Instantiate model Lasso model with an appropriate alpha value\n",
    "lasso = Lasso(alpha=0.001)    \n",
    "\n",
    "# 2.c. Implement the machine learning model with X and y\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0)\n",
    "\n",
    "lasso.fit(X_train, y_train)\n",
    "\n",
    "print(\"R2 Training score for Lasso model: {:.2f}\".format(r2_score(y_train, lasso.predict(X_train))))\n",
    "print(\"R2 Valdiation score for Lasso model: {:.2f}\".format(r2_score(y_test, lasso.predict(X_test))))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b606236",
   "metadata": {},
   "source": [
    "*ANSWER HERE*\n",
    "In practice, an R^2 score above 0.7 or 0.8 is often considered good, indicating that the model captures a substantial amount of the variance in the data. However the Ridge model(at alpha - from 0.01 to 37 gives its maximum value of around .54) and same for lasso(at alpha = 0.01). However, as compared to Linear Regession, both the model are not giving good output."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
